---
title: "Comprehensive Random Analysis of Covid-19"
output: 
  
  html_document:
    code_folding: hide
date: "2023-01-24"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
run_base <- FALSE
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

```{r include = FALSE}
library(DSIWastewater)
library(dplyr)
library(ggplot2)
library(tidyr)
set.seed(123)
```


```{r adeline data prep, include = FALSE}
library(data.table)
load("../../dhs_code/Normalization/data/data.H12.backup.RData")
'%!in%' <- function(x,y)!('%in%'(x,y))
'%!like%' <- function(x,y)!('%like%'(x,y))
data.H12<-data.H12.backup
### Prep H12 data 
names(data.H12) <- c("wwtp_name", "epaid", "zipcode", "county_names", "state", "capacity_mgd", "population_served", "sample_type", "composite_freq", "sample_matrix", "sample_location", "sample_location_specify", "sample_id", "wwtp_comments", "concentration_method", "extraction_method", "pcr_type", "lod_ref", "quant_stan_type", "quant_stan_ref", "inhibition_method", "n1_sars_cov2_units", "n1_sars_cov2_conc", "n1_sars_cov2_lod", "n1_sars_cov2_error", "n1_ntc_amplify", "n1_num_ntc_amplify", "n1_num_no_target_control", "n1_lod", "n1_loq", "n2_sars_cov2_units", "n2_sars_cov2_conc", "n2_sars_cov2_lod", "n2_sars_cov2_error", "n2_ntc_amplify", "n2_num_ntc_amplify", "n2_num_no_target_control", "n2_lod", "n2_loq", "avg_sars_cov2_conc", "avg_sars_cov2_below_lod", "pmmov_conc", "hf183_conc", "bcov_rec_rate", "inhibition_detect", "inhibition_adjust", "analytical_comments", "sample_collect_date", "sample_collect_time", "test_result_date", "average_flow_rate", "equiv_sewage_amt", "tss", "ph", "bod", "conductivity", "temperature", "do", "bcov_spike_conc")

# Clean samples
data.H12 <- data.H12 %>% filter(wwtp_name %!like% "Madison-P")
data.H12 <- data.H12 %>% filter(wwtp_name %!in% c("Keshena", "Lac du Flambeau", "Menomonie", "Mondovi", "Neopit", "Red Cliff", "Wolf", "Wolf River"))
data.H12 <- data.H12 %>% filter(wwtp_name != "")

# Fix wwtp names
data.H12$wwtp_name <- gsub(" WWTP", "", data.H12$wwtp_name)
data.H12$wwtp_name <- gsub("WWTP", "", data.H12$wwtp_name)
data.H12$wwtp_name <- gsub("WWTF", "", data.H12$wwtp_name)
data.H12$wwtp_name <- gsub("BlackRiverFalls", "Black River Falls", data.H12$wwtp_name)
data.H12$wwtp_name <- gsub(" Metro", "", data.H12$wwtp_name)
data.H12$wwtp_name <- gsub("WI Rapids", "Wisconsin Rapids", data.H12$wwtp_name)
#print("List wwtps investigated (in H12 extract):")
#levels(as.factor(data.H12$wwtp_name))

# Add ID
data.H12$sample_collect_date<-as.Date(data.H12$sample_collect_date, format="%m/%d/%Y")
data.H12$ID<-paste0(data.H12$wwtp_name, data.H12$sample_collect_date)


data.meta<-read.table("../../dhs_code/Normalization/data/WWDataRequestDNR.csv", sep = ",", h=T)
data.meta<-data.meta %>% filter(lab_submitter == "SLH")
### Preparation of metadata
# Fix wwtp names
data.meta$wwtp_name<-gsub(" WWTF", "", data.meta$wwtp_name)
data.meta$wwtp_name<-gsub(" WWTP", "", data.meta$wwtp_name)
data.meta$wwtp_name<-gsub(" Sewage Utility", "", data.meta$wwtp_name)
data.meta$wwtp_name<-gsub(" WW Utility", "", data.meta$wwtp_name)
data.meta$wwtp_name<-gsub(" Wastewater Utility", "", data.meta$wwtp_name)
data.meta$wwtp_name<-gsub(" WPCF", "", data.meta$wwtp_name)
data.meta$wwtp_name<-gsub(" MSD", "", data.meta$wwtp_name)
data.meta$wwtp_name<-gsub(" Utilities", "", data.meta$wwtp_name)
data.meta$wwtp_name<-gsub(" Municipal Utility", "", data.meta$wwtp_name)
data.meta$wwtp_name<-gsub(" Water Works", "", data.meta$wwtp_name)
#levels(as.factor(data.meta$wwtp_name))
#print("List wwtps with metadata (should be the same than above):")
#levels(as.factor(data.H12$wwtp_name))

# Add ID
data.meta$sample_collect_date <- as.Date(data.meta$sample_collect_date, format="%m/%d/%Y")
data.meta$ID <- paste0(data.meta$wwtp_name, data.meta$sample_collect_date)

# Metadata #1
data.meta.1 <- unique(data.meta[, c("ID", "sars_cov2_adj_load", "cases", "case_rate")])

# Metadata #2
data.meta.2 <- data.meta[, c("ID", "result_amt", "storet_parm_desc", "parm_unit_type")]
data.meta.2$storet_parm_desc <- gsub("CBOD5", "BOD5", data.meta.2$storet_parm_desc)
data.meta.2$storet_parm_desc <- gsub("BOD5, Total", "BOD5", data.meta.2$storet_parm_desc)
data.meta.2$storet_parm_desc <- gsub("Suspended Solids, Total", "TSS", data.meta.2$storet_parm_desc)
data.meta.2 <- reshape2::dcast(data.meta.2, ID~storet_parm_desc, value.var = "result_amt", fun.aggregate= function(x) if(length(x) == 0) NA_real_ else mean(x, na.rm = TRUE))
data.meta.2 <- data.meta.2 %>% select("ID", "BOD5", "Flow Rate", "TSS")

# Metadata - merged #1 and #2
data.meta <- left_join(data.meta.1, data.meta.2, by="ID")

data <- inner_join(data.H12, data.meta, by="ID")
#data<-data.H12

data <-data %>% 
  filter(pcr_type == "qPCR")%>%#3.7, 2.6, 2,1
  filter(n1_sars_cov2_lod == "No" | n2_sars_cov2_lod == "No")

data$avg_sars_cov2_conc <- as.numeric(as.character(data$avg_sars_cov2_conc))
data$average_flow_rate <- as.numeric(as.character(data$average_flow_rate))
data$population_served <- as.numeric(as.character(data$population_served))
data$pmmov_conc <- as.numeric(as.character(data$pmmov_conc))
data$hf183_conc <- as.numeric(as.character(data$hf183_conc))
data$bcov_rec_rate <- as.numeric(as.character(data$bcov_rec_rate))
data$tss <- as.numeric(as.character(data$tss))
data$ph <- as.numeric(as.character(data$ph))
data$temperature <- as.numeric(as.character(data$temperature))
data$conductivity <- as.numeric(as.character(data$conductivity))
data$sars_cov2_adj_load <- as.numeric(as.character(data$sars_cov2_adj_load))
data$cases <- as.numeric(as.character(data$cases))
data$case_rate <- as.numeric(as.character(data$case_rate))
data$BOD5 <- as.numeric(as.character(data$BOD5))
data$`Flow Rate` <- as.numeric(as.character(data$`Flow Rate`))
data$TSS <- as.numeric(as.character(data$TSS))

data$wwtp_name <- as.factor(data$wwtp_name)

#data <- data%>%
#  mutate(across(c("pmmov_conc", "bcov_rec_rate", "average_flow_rate", "TSS") , ~log(.x)))%>%
#  filter(across(c("pmmov_conc", "bcov_rec_rate", "average_flow_rate", "TSS") , ~is.finite(.x)))

####GROSS CODE
# Difference between value and average of the value for a given WWTP

# Difference between WW and case data 
data$difference.log <- log10(data$avg_sars_cov2_conc+1)-(data$case_rate)

mean.indpt<-data %>%
  group_by(wwtp_name) %>%
  summarise(mean_pmmov_conc = mean(pmmov_conc, na.rm=TRUE), 
          mean_bcov_rec_rate = mean(bcov_rec_rate, na.rm=TRUE), 
          mean_average_flow_rate = mean(average_flow_rate, na.rm=TRUE), 
          mean_ph = mean(ph, na.rm=TRUE), 
          mean_temperature = mean(temperature, na.rm=TRUE), 
          mean_BOD5 = mean(BOD5, na.rm=TRUE), 
          mean_TSS = mean(TSS, na.rm=TRUE))

data.select.1<-left_join(data, mean.indpt, by="wwtp_name")
data.select.1$diff_pmmov_conc<-log10(data.select.1$mean_pmmov_conc+1)-log10(data.select.1$pmmov_conc+1)
data.select.1$diff_bcov_rec_rate<-data.select.1$mean_bcov_rec_rate - data.select.1$bcov_rec_rate
data.select.1$diff_average_flow_rate<-data.select.1$mean_average_flow_rate - data.select.1$average_flow_rate
data.select.1$diff_ph<-data.select.1$mean_ph - data.select.1$ph
data.select.1$diff_temperature<- data.select.1$mean_temperature - data.select.1$temperature
data.select.1$diff_BOD5<-data.select.1$mean_BOD5 - data.select.1$BOD5
data.select.1$diff_TSS<-data.select.1$mean_TSS - data.select.1$TSS

#left_join(data, mean.indpt, by="wwtp_name")

data.select.1<-data.select.1 %>% 
  select(wwtp_name, diff_pmmov_conc, diff_bcov_rec_rate, diff_average_flow_rate, diff_ph, diff_temperature, diff_BOD5, diff_TSS, difference.log, case_rate, avg_sars_cov2_conc)%>% 
  #mutate_all(~ifelse(is.nan(.), NA, .)) %>%
  filter(!is.na(difference.log))




# Center scale the data 
data.select.1[data.select.1 == "NaN"] <- NA
data.select.1.sc<-scale(data.select.1 %>% 
 select(diff_pmmov_conc, diff_bcov_rec_rate, diff_average_flow_rate, diff_ph, diff_temperature, diff_BOD5, 
        diff_TSS, difference.log), center = TRUE, scale = TRUE)
data.select.1.sc<-cbind(data.select.1$wwtp_name, 
                        data.select.1$case_rate,
                        data.select.1$avg_sars_cov2_conc, 
                        as.data.frame(data.select.1.sc))

names(data.select.1.sc)[1:3] <- c("wwtp_name", "case_rate", "avg_sars_cov2_conc")

```


This is a response to Adeline document about using random forest to explore the factors
related to Covid-19. We liked the ideas included and hoped to extend it by exploring
specific hypothesis about the question.

Motivation:
Adeline document cheeked to answer the importance of wastewater covariants by using
a random forest on the difference of N1 and cases. This is mathematically equivalent
on running the forest on the residuals of the model Case = covid concentration This
creates a problem because this difference remains strongly correlated with both the 
case signal and the wastewater signal meaning that large deviance is more explained
by deviance in two source vectors then in the need for another variable. Bellow
shows both the correlation and scatter plots. we hope to show two rigorous hypothesis
that don't have the same limitations.


```{r adeline correlation}
title_prep <- function(x)paste("Correlation is", round(x,5))
case_cor <- cor(data.select.1.sc$difference.log, data.select.1.sc$case_rate, use = "na.or.complete")
waste_cor <- cor(data.select.1.sc$difference.log, data.select.1.sc$avg_sars_cov2_conc, use = "na.or.complete")

{
layout(matrix(1:2, 1, 2, byrow = TRUE))
plot(x = data.select.1.sc$case_rate, y = data.select.1.sc$difference.log, 
     xlab = "case rate", ylab = "log(Covid-19 concentration) - case rate", main = title_prep(case_cor))
plot(x = log(data.select.1.sc$avg_sars_cov2_conc + 1), y = data.select.1.sc$difference.log, 
     xlab = "log(Covid-19 concentration)", ylab = "", main = title_prep(waste_cor))
}


#summary(lm(log(case_rate + 1) ~  log(avg_sars_cov2_conc + 1):wwtp_name + wwtp_name, data = data.select.1.sc))
```

Hypothesis one: there is a linear relationship between the log of N1 and log of cases. The covariates change the intercept of the relationship but there is always a base relationship.

A quick look at the data makes this look plausible with a thick cloud that looks like it could be explained by many perpendicular linear models.  There is a sequence of points where case_rate was 0 that do not fit the trend but these can be removed or fixed using the 7 day smoothing of the case data. Furthermore the residuals look normal which helps support that conclusion. 




```{r lm cont}


lm_model = lm(log(avg_sars_cov2_conc + 1) ~ log(case_rate + 1) , 
              data = data.select.1.sc, 
              na.action = na.exclude)

data.select.1.sc$resid = resid(lm_model)
  
  #(data.select.1.sc$case_rate -  coef(lm_model)[1])/data.select.1.sc$sars_cov2_adj_load
#? don't show. mention off hand instead
hist(data.select.1.sc$resid, main = "Histogram of Linear Model Residuals Appear Normal", xlab = "residuals")
base_df <- data.select.1.sc%>%
  rename(conf_case = case_rate)

summary(lm_model)

data.select.2.sc <- data.select.1.sc
```

Because the goal is to capture variation in wastewater that is not in cases we used the residual of the model N1 ~ cases. This method has a much smaller correlation of the two source vectors giving results much better at capturing the important covariants
```{r}
resid_case_cor <- cor(data.select.1.sc$resid, 
                      data.select.1.sc$case_rate, 
                      use = "na.or.complete")
resid_waste_cor <- cor(data.select.1.sc$resid, 
                       data.select.1.sc$avg_sars_cov2_conc, 
                       use = "na.or.complete")

{
layout(matrix(1:4, 2, 2, byrow = TRUE))
plot(x = data.select.1.sc$case_rate, y = data.select.1.sc$difference.log, 
     xlab = "", ylab = "log(conc) - case rate", main = title_prep(case_cor))
plot(x = log(data.select.1.sc$avg_sars_cov2_conc), y = data.select.1.sc$difference.log, 
     xlab = "", ylab = "", main = title_prep(waste_cor))
plot(x = log(data.select.1.sc$case_rate + 1), y = data.select.1.sc$resid, 
     xlab = "case_rate", ylab = "linear model resid", main = title_prep(resid_case_cor))
plot(x = log(data.select.1.sc$avg_sars_cov2_conc), y = data.select.1.sc$resid,
     xlab = "log(Covid-19 concentration)", ylab = "", main = title_prep(resid_waste_cor))
}


```

```{r ploting func}
imp_ploting <- function(temp_mod){
  ImpData <- as.data.frame(randomForest::importance(temp_mod))
  ImpData$Var.Names <- row.names(ImpData)


ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank())
}
```

```{r random forest fitting}
library(randomForest)
library(randomForestExplainer)
na.controlmethod <- na.roughfix#na.exclude#


adeline_df <- data.select.2.sc%>%
  select(-case_rate, -avg_sars_cov2_conc)
    

ade_mod <- randomForest(difference.log ~ ., data = adeline_df, ntree=500,
                    na.action = na.controlmethod, importance=TRUE, keep.inbag=TRUE)

ade_mod

resid_df <- data.select.2.sc%>%
  select(-case_rate, -avg_sars_cov2_conc, - difference.log)
    

resid_mod <- randomForest(resid ~ ., data = resid_df, ntree=500,
                    na.action = na.controlmethod, importance=TRUE, keep.inbag=TRUE) 

resid_mod

case_df <- data.select.2.sc%>%
  select(-difference.log, -avg_sars_cov2_conc, - resid)
case_mod <- randomForest(case_rate ~ ., data = case_df, ntree=500,
                    na.action = na.controlmethod, importance=TRUE, keep.inbag=TRUE) 

case_mod

data.frame('target variable' = c('difference.log', 'lm resid', 'case_rate'),
           'mean squared residuals' = c("0.7474236", " 0.29453", "4610.051"),
           '% var explained' = c("25.24", "21.75", "25.73"))
```


```{r compare imp plots}
addSmallLegend <- function(myPlot, pointSize = 1.4, textSize = 8, spaceLegend = 0.21) {
    myPlot +
        guides(shape = guide_legend(override.aes = list(size = pointSize)),
               color = guide_legend(override.aes = list(size = pointSize))) +
        theme(legend.title = element_text(size = textSize), 
              legend.text  = element_text(size = textSize),
              legend.key.size = unit(spaceLegend, "lines"))
}

#3 lines color based on group

ImpData_1 <- as.data.frame(randomForest::importance(ade_mod))%>%
  mutate(IncNodePurity = IncNodePurity/mean(ade_mod$mse))
ImpData_1$Var.Names <- row.names(ImpData_1)
ImpData_2 <- as.data.frame(randomForest::importance(case_mod))%>%
  mutate(IncNodePurity = IncNodePurity/mean(case_mod$mse))
ImpData_2$Var.Names <- row.names(ImpData_2)
ImpData_3 <- as.data.frame(randomForest::importance(resid_mod))%>%
  mutate(IncNodePurity = IncNodePurity/mean(resid_mod$mse))
ImpData_3$Var.Names <- row.names(ImpData_3)


p <- ggplot(mapping  = aes(x=Var.Names, y=`%IncMSE`))+
  geom_segment( mapping  = aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`, color = "case rate"),
                data = ImpData_2, position = position_nudge(x = 0.2)) +
  geom_point(mapping  = aes(size = IncNodePurity, color = "case rate"), alpha=0.6, data = ImpData_2,
             position = position_nudge(x = 0.2))+
  geom_segment( mapping  = aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`, color = "difference"),
                data = ImpData_1) +
  geom_point(mapping  = aes(size = IncNodePurity, color = "difference"), alpha=0.6, data = ImpData_1)+
  geom_segment( mapping  = aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`, color = "resid"),
                data = ImpData_3, position = position_nudge(x = -0.2)) +
  geom_point(mapping  = aes(size = IncNodePurity, color = "resid"), alpha=0.6, data = ImpData_3, 
                position = position_nudge(x = -0.2))+
  theme_light() +
  labs(color = "Model")+
  xlab("variable name")+
  ggtitle("Tree Model Feature Importance Shows Similarities")+
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank())
addSmallLegend(p)
```








We hope this shows the value of using the residual over the log difference.
A key component missing so far is how to use this analysis. One method is to look
at the partial difference plots. A partial difference plot looks at each independent
variable and sees how the model output changes on average when you change that variable.
This can show how important each variable is and hopefully how they relate to to the
target variable. below is the partial difference plot for Adeline's model with each
graphic is coloring a difference main data. This is also done with the residual model


```{r ade  partial plots temp}


graph_df <- adeline_df%>%
  na.roughfix()
library(forestFloor)
ff = forestFloor(ade_mod, graph_df, calc_np=T)
for(i in 1:8){
  Col = fcol(ff, cols = i, outlier.lim = 2.5)
  plot(ff, col=Col, plot_GOF = T)
}
```

```{r linear model partial plots}
graph_df2 <- resid_df%>%
  na.roughfix()

ff = forestFloor(resid_mod, graph_df2, calc_np=T)
for(i in 1:8){
  Col = fcol(ff, cols = i, outlier.lim = 2.5)
  plot(ff,col=Col, plot_GOF = T)
}

```


One issue in interpreting these plots is that a leveling of the graph could mean
that the trend stops or that there is a lack of data in that area to split trees.
looking at Adeline's plots we see a clear strong linear trend in flow. This seems
to reduce variance by around 5% but makes the correlation worse. overall this is not convening result.

```{r}
x = randomForest::partialPlot(ade_mod, graph_df, 
                              x.var = names(graph_df)[[4]],
                              plot = FALSE)
{
plot(x, 
     xlab = names(graph_df)[[4]], 
     ylab = "conditional change of forest model output",
     main = "difference is proportional to flow")
abline(a = -40, b = -90)
}


prop_df <- data.select.2.sc%>%
  rename(flow = diff_average_flow_rate)%>%
  mutate(flow_mod_diff = 90*flow + 40 + difference.log,
         cond_flow_mod_diff = ifelse( 0 <= flow & flow <= 1,
                              90*flow + 40 + difference.log,
                              difference.log),
         flow_mod_conc = 90*flow + 40 + log(avg_sars_cov2_conc + 1),
         cond_flow_mod_conc = ifelse( 0 <= flow & flow <= 1,
                              90*flow + 40 + log(avg_sars_cov2_conc + 1),
                              log(avg_sars_cov2_conc + 1))
         )

var_sum_df <- prop_df%>%
  summarise(name = "variance",
            'base value' = var(difference.log), 
            'linear normalization' = var(flow_mod_diff),
            'conditional normalization' = var(cond_flow_mod_diff))



cor_sum_df <- prop_df%>%
  summarise(name = "corelation",
            'base value' = cor(log(avg_sars_cov2_conc + 1), case_rate), 
            'linear normalization' = cor(flow_mod_conc, case_rate),
            'conditional normalization' = cor(cond_flow_mod_conc, case_rate))


rbind(var_sum_df, cor_sum_df)


```



using the linear model residual as a target the bcov rate has a clear linear trend
to try to normalize. This gives a reduction of variance of around 7% and very
slightly increases the correlation. This is still small enough to not be convincing

```{r}

x = randomForest::partialPlot(resid_mod, graph_df2, 
                              x.var = names(graph_df2)[[3]],
                              plot = FALSE)
{
plot(x, 
     xlab = names(graph_df2)[[3]], 
     ylab = "conditional change of forest model output",
     main = "difference is proportional to bcov")
abline(a = 0, b = -.07)
}

prop_df <- data.select.2.sc%>%
  rename(bcov = diff_bcov_rec_rate)%>%
  mutate(bcov_mod_diff = .07*bcov  + resid,
         cond_bcov_mod_diff = ifelse( -2 <= bcov,
                              .07*bcov + resid,
                              resid),
         bcov_mod_conc = .07*bcov  + log(avg_sars_cov2_conc + 1),
         cond_bcov_mod_conc = ifelse( -2 <= bcov,
                              .07*bcov + log(avg_sars_cov2_conc + 1),
                              log(avg_sars_cov2_conc + 1))
         )

var_sum_df <- prop_df%>%
  summarise(name = "variance",
            'base value' = var(resid), 
            'linear normalization' = var(bcov_mod_diff),
            'conditional normalization' = var(cond_bcov_mod_diff))


cor_sum_df <- prop_df%>%
  summarise(name = "corelation",
            'base value' = cor(log(avg_sars_cov2_conc + 1), case_rate), 
            'linear normalization' = cor(bcov_mod_conc, case_rate),
            'conditional normalization' = cor(cond_bcov_mod_conc, case_rate))


rbind(var_sum_df, cor_sum_df)
```

